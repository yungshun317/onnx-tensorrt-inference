{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "894a8281-5210-42ba-80d3-ef44925b33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yungshun317/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['custom',\n",
       " 'yolov5l',\n",
       " 'yolov5l6',\n",
       " 'yolov5m',\n",
       " 'yolov5m6',\n",
       " 'yolov5n',\n",
       " 'yolov5n6',\n",
       " 'yolov5s',\n",
       " 'yolov5s6',\n",
       " 'yolov5x',\n",
       " 'yolov5x6']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.hub.list(github=\"ultralytics/yolov5\", trust_repo='check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3162e26f-152e-4e08-8c92-ff64b71a61ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yungshun317/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-10-8 Python-3.12.3 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16072MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:00<00:00, 18.7MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoShape(\n",
      "  (model): DetectMultiBackend(\n",
      "    (model): DetectionModel(\n",
      "      (model): Sequential(\n",
      "        (0): Conv(\n",
      "          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv(\n",
      "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (4): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): Conv(\n",
      "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (6): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): Conv(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (8): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): SPPF(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (10): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        (12): Concat()\n",
      "        (13): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (14): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (15): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        (16): Concat()\n",
      "        (17): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (18): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (19): Concat()\n",
      "        (20): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (21): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (22): Concat()\n",
      "        (23): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (24): Detect(\n",
      "          (m): ModuleList(\n",
      "            (0): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (2): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Check model information from `hubconf.py` in `ultralytics/yolov5`\n",
    "# Official model will be downloaded to local as `yolov5s.pt`\n",
    "yolo = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n",
    "print(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0569b5e4-0fa4-43e5-a935-0fda3bb644da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yungshun317/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>745.578735</td>\n",
       "      <td>48.471649</td>\n",
       "      <td>1142.671021</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>0.869008</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>124.810516</td>\n",
       "      <td>197.328827</td>\n",
       "      <td>844.369141</td>\n",
       "      <td>716.658569</td>\n",
       "      <td>0.630537</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>441.253235</td>\n",
       "      <td>439.367798</td>\n",
       "      <td>498.379883</td>\n",
       "      <td>708.573730</td>\n",
       "      <td>0.616540</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>594.089294</td>\n",
       "      <td>377.306091</td>\n",
       "      <td>635.425720</td>\n",
       "      <td>437.154663</td>\n",
       "      <td>0.274228</td>\n",
       "      <td>67</td>\n",
       "      <td>cell phone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin         xmax        ymax  confidence  class  \\\n",
       "0  745.578735   48.471649  1142.671021  720.000000    0.869008      0   \n",
       "1  124.810516  197.328827   844.369141  716.658569    0.630537      0   \n",
       "2  441.253235  439.367798   498.379883  708.573730    0.616540     27   \n",
       "3  594.089294  377.306091   635.425720  437.154663    0.274228     67   \n",
       "\n",
       "         name  \n",
       "0      person  \n",
       "1      person  \n",
       "2         tie  \n",
       "3  cell phone  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Image\n",
    "image = 'https://ultralytics.com/images/zidane.jpg'\n",
    "\n",
    "# Inference\n",
    "results = yolo(image)\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6908923e-5a86-4524-8b5d-fb1099d1aa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1: 720x1280 2 persons, 1 tie, 1 cell phone\n",
      "Speed: 2283.7ms pre-process, 24.2ms inference, 50.8ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86dce1de-38ed-42fa-90e8-4f57788bcb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "DetectionModel                                [1, 25200, 85]            --\n",
       "â”œâ”€Sequential: 1-1                             --                        --\n",
       "â”‚    â””â”€Conv: 2-1                              [1, 32, 320, 320]         --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-1                       [1, 32, 320, 320]         (3,488)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-2                         [1, 32, 320, 320]         --\n",
       "â”‚    â””â”€Conv: 2-2                              [1, 64, 160, 160]         --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-3                       [1, 64, 160, 160]         (18,496)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-4                         [1, 64, 160, 160]         --\n",
       "â”‚    â””â”€C3: 2-3                                [1, 64, 160, 160]         --\n",
       "â”‚    â”‚    â””â”€Conv: 3-5                         [1, 32, 160, 160]         (2,080)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-6                   [1, 32, 160, 160]         (10,304)\n",
       "â”‚    â”‚    â””â”€Conv: 3-7                         [1, 32, 160, 160]         (2,080)\n",
       "â”‚    â”‚    â””â”€Conv: 3-8                         [1, 64, 160, 160]         (4,160)\n",
       "â”‚    â””â”€Conv: 2-4                              [1, 128, 80, 80]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-9                       [1, 128, 80, 80]          (73,856)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-10                        [1, 128, 80, 80]          --\n",
       "â”‚    â””â”€C3: 2-5                                [1, 128, 80, 80]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-11                        [1, 64, 80, 80]           (8,256)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-12                  [1, 64, 80, 80]           (82,176)\n",
       "â”‚    â”‚    â””â”€Conv: 3-13                        [1, 64, 80, 80]           (8,256)\n",
       "â”‚    â”‚    â””â”€Conv: 3-14                        [1, 128, 80, 80]          (16,512)\n",
       "â”‚    â””â”€Conv: 2-6                              [1, 256, 40, 40]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-15                      [1, 256, 40, 40]          (295,168)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-16                        [1, 256, 40, 40]          --\n",
       "â”‚    â””â”€C3: 2-7                                [1, 256, 40, 40]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-17                        [1, 128, 40, 40]          (32,896)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-18                  [1, 128, 40, 40]          (492,288)\n",
       "â”‚    â”‚    â””â”€Conv: 3-19                        [1, 128, 40, 40]          (32,896)\n",
       "â”‚    â”‚    â””â”€Conv: 3-20                        [1, 256, 40, 40]          (65,792)\n",
       "â”‚    â””â”€Conv: 2-8                              [1, 512, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-21                      [1, 512, 20, 20]          (1,180,160)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-22                        [1, 512, 20, 20]          --\n",
       "â”‚    â””â”€C3: 2-9                                [1, 512, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-23                        [1, 256, 20, 20]          (131,328)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-24                  [1, 256, 20, 20]          (655,872)\n",
       "â”‚    â”‚    â””â”€Conv: 3-25                        [1, 256, 20, 20]          (131,328)\n",
       "â”‚    â”‚    â””â”€Conv: 3-26                        [1, 512, 20, 20]          (262,656)\n",
       "â”‚    â””â”€SPPF: 2-10                             [1, 512, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-27                        [1, 256, 20, 20]          (131,328)\n",
       "â”‚    â”‚    â””â”€MaxPool2d: 3-28                   [1, 256, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€MaxPool2d: 3-29                   [1, 256, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€MaxPool2d: 3-30                   [1, 256, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-31                        [1, 512, 20, 20]          (524,800)\n",
       "â”‚    â””â”€Conv: 2-11                             [1, 256, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-32                      [1, 256, 20, 20]          (131,328)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-33                        [1, 256, 20, 20]          --\n",
       "â”‚    â””â”€Upsample: 2-12                         [1, 256, 40, 40]          --\n",
       "â”‚    â””â”€Concat: 2-13                           [1, 512, 40, 40]          --\n",
       "â”‚    â””â”€C3: 2-14                               [1, 256, 40, 40]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-34                        [1, 128, 40, 40]          (65,664)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-35                  [1, 128, 40, 40]          (164,096)\n",
       "â”‚    â”‚    â””â”€Conv: 3-36                        [1, 128, 40, 40]          (65,664)\n",
       "â”‚    â”‚    â””â”€Conv: 3-37                        [1, 256, 40, 40]          (65,792)\n",
       "â”‚    â””â”€Conv: 2-15                             [1, 128, 40, 40]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-38                      [1, 128, 40, 40]          (32,896)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-39                        [1, 128, 40, 40]          --\n",
       "â”‚    â””â”€Upsample: 2-16                         [1, 128, 80, 80]          --\n",
       "â”‚    â””â”€Concat: 2-17                           [1, 256, 80, 80]          --\n",
       "â”‚    â””â”€C3: 2-18                               [1, 128, 80, 80]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-40                        [1, 64, 80, 80]           (16,448)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-41                  [1, 64, 80, 80]           (41,088)\n",
       "â”‚    â”‚    â””â”€Conv: 3-42                        [1, 64, 80, 80]           (16,448)\n",
       "â”‚    â”‚    â””â”€Conv: 3-43                        [1, 128, 80, 80]          (16,512)\n",
       "â”‚    â””â”€Conv: 2-19                             [1, 128, 40, 40]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-44                      [1, 128, 40, 40]          (147,584)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-45                        [1, 128, 40, 40]          --\n",
       "â”‚    â””â”€Concat: 2-20                           [1, 256, 40, 40]          --\n",
       "â”‚    â””â”€C3: 2-21                               [1, 256, 40, 40]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-46                        [1, 128, 40, 40]          (32,896)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-47                  [1, 128, 40, 40]          (164,096)\n",
       "â”‚    â”‚    â””â”€Conv: 3-48                        [1, 128, 40, 40]          (32,896)\n",
       "â”‚    â”‚    â””â”€Conv: 3-49                        [1, 256, 40, 40]          (65,792)\n",
       "â”‚    â””â”€Conv: 2-22                             [1, 256, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€Conv2d: 3-50                      [1, 256, 20, 20]          (590,080)\n",
       "â”‚    â”‚    â””â”€SiLU: 3-51                        [1, 256, 20, 20]          --\n",
       "â”‚    â””â”€Concat: 2-23                           [1, 512, 20, 20]          --\n",
       "â”‚    â””â”€C3: 2-24                               [1, 512, 20, 20]          --\n",
       "â”‚    â”‚    â””â”€Conv: 3-52                        [1, 256, 20, 20]          (131,328)\n",
       "â”‚    â”‚    â””â”€Sequential: 3-53                  [1, 256, 20, 20]          (655,872)\n",
       "â”‚    â”‚    â””â”€Conv: 3-54                        [1, 256, 20, 20]          (131,328)\n",
       "â”‚    â”‚    â””â”€Conv: 3-55                        [1, 512, 20, 20]          (262,656)\n",
       "â”‚    â””â”€Detect: 2-25                           [1, 25200, 85]            --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-56                  --                        (229,245)\n",
       "===============================================================================================\n",
       "Total params: 7,225,885\n",
       "Trainable params: 0\n",
       "Non-trainable params: 7,225,885\n",
       "Total mult-adds (Units.GIGABYTES): 8.24\n",
       "===============================================================================================\n",
       "Input size (MB): 4.92\n",
       "Forward/backward pass size (MB): 206.37\n",
       "Params size (MB): 28.90\n",
       "Estimated Total Size (MB): 240.19\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Peek inside the nested model\n",
    "summary(yolo.model.model, input_size=(1, 3, 640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f46e6-7a8c-4238-9270-52d108994097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `export.py` for converting `yolov5s.pt` to ONNX from Ultralytics's YOLOv5 repository\n",
    "\n",
    "# !cd yolov5\n",
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "# !cd yolov5\n",
    "# !pip3 install ultralytics\n",
    "# !python export.py --weights yolov5s.pt --workspace 2048 --data data/coco.yaml --include onnx --opset 12\n",
    "\n",
    "# This will output a `yolov5s.onnx` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99e8eb-5ae3-462b-98bb-869add253cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /usr/src/tensorrt/bin/trtexec --onnx=yolov5s.onnx --saveEngine=/tensorfl_vision/Onnx-Inference-Yolov5/models_engine/yolov5s.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913a9fd-dd89-4d02-b5ca-0ce3b1f355ce",
   "metadata": {},
   "source": [
    "# YOLOv5\n",
    "\n",
    "1: is due to that only 1 image has been processed\n",
    "25200: is the number of anchors\n",
    "85: is [x, y, w, h, objectness, 80 coco classes]\n",
    "\n",
    "The 85 in the output tensor [1, 25556, 85] corresponds to the number of classes plus the bounding box coordinates and the objectness score for each prediction. In YOLOv5, the output tensor typically has the shape [batch_size, number_of_anchors, 4 + 1 + number_of_classes], where:\n",
    "\n",
    "4 represents the bounding box coordinates (x, y, width, height),\n",
    "1 represents the objectness score, and\n",
    "number_of_classes is the number of classes the model is trained to detect.\n",
    "\n",
    "To make sense of these outputs and convert them into a more usable form (bounding boxes, class IDs, and scores), you'll typically need to apply some post-processing steps. Hereâ€™s a brief overview:\n",
    "- Apply a sigmoid function to the objectness scores and class predictions to convert logits to probabilities.\n",
    "- Filter out predictions with objectness scores below a certain threshold to reduce the number of detections, as many will be low confidence.\n",
    "- Apply Non-Max Suppression (NMS): Since your model may predict multiple overlapping boxes for a single object, NMS helps in selecting the most probable bounding box while discarding the rest.\n",
    "\n",
    "1. `cv.dnn.readNetFromONNX()`\n",
    "2. `cv.dnn.Net.getUnconnectedOutLayersNames()`: Returns names of layers with unconnected outputs.\n",
    "3. `cv.dnn.Net.forward(outputName)`: Runs forward pass to compute output of layer with name `outputName`. By default runs forward pass for the whole network.\n",
    "4. `cv.dnn.NMSBoxes(bboxes, scores, score_threshold, nms_threshold, eta=1.0, top_k=0)`: Performs non maximum suppression given boxes and corresponding scores. `eta` is a coefficient in adaptive threshold formula: $nms\\_threshold_{i+1}=\\etaâ‹…nms\\_threshold_{i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e91e821-1535-4fe6-8f9c-8a96acfa4352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV DNN YOLOv5 Inference Time: 0.07496237754821777\n",
      "OpenCV DNN YOLOv5 Inference Time: 0.0379633903503418\n",
      "OpenCV DNN YOLOv5 Inference Time: 0.03557896614074707\n",
      "OpenCV DNN YOLOv5 Inference Time: 0.03752326965332031\n",
      "OpenCV DNN YOLOv5 Inference Time: 0.03410220146179199\n",
      "OpenCV DNN YOLOv5 Inference Time: 0.034317731857299805\n",
      "OpenCV DNN YOLOv5 Inference Time: 0.038039207458496094\n",
      "OpenCV DNN YOLOv5 Inference Time: 0.034429073333740234\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Packages:\n",
    "    cv2\n",
    "    numpy\n",
    "    time\n",
    "    os\n",
    "\"\"\"\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "'''\n",
    "    Target: Make inference With YOLOv5 Onnx model on set of images\n",
    "    param[1]: path_to_images\n",
    "    param[2]: path_to_model\n",
    "    param[3]: image_height\n",
    "    param[4]: image_width\n",
    "    param[5]: confidence threshold\n",
    "    param[6]: score threshold\n",
    "    param[7]: nonmax suppression threshold\n",
    "    param[8]: path_to_classes \n",
    "'''\n",
    "class YOLOv5:\n",
    "    # Initialize Declaration\n",
    "    def __init__(self, path_to_images, path_to_model, image_width, image_height, conf_threshold, score_threshold, nms_threshold, path_to_classes):\n",
    "        # Declare parameters\n",
    "        self.path_to_images = path_to_images\n",
    "        self.path_to_model = path_to_model\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.score_threshold = score_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.path_to_classes = path_to_classes\n",
    "\n",
    "    # Python prebuilt `__call__` function\n",
    "    '''\n",
    "        target: Load images & ONNX model\n",
    "        Return type : output list\n",
    "    '''\n",
    "    def __call__(self):\n",
    "        for image in os.listdir(self.path_to_images):\n",
    "            if image.endswith('.jpg') or image.endswith('.jpeg') or image.endswith('.png'):\n",
    "                # Full absolute path to image\n",
    "                image_full_path = os.path.join(self.path_to_images, image)\n",
    "                # Read image\n",
    "                image = cv2.imread(image_full_path)\n",
    "                # Load ONNX model\n",
    "                network = cv2.dnn.readNetFromONNX(self.path_to_model)\n",
    "                # GPU configuration\n",
    "                network.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "                network.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "                # Read classess\n",
    "                classes = self.class_name()\n",
    "                # Detection function\n",
    "                self.detection(image, network, classes)\n",
    "\n",
    "    # Read labels of COCO \n",
    "    '''\n",
    "        target: read class labels\n",
    "        Out: [classes list]\n",
    "    '''\n",
    "    def class_name(self):\n",
    "        # List of classes\n",
    "        classes = []\n",
    "        file = open(self.path_to_classes, 'r')\n",
    "\n",
    "        while True:\n",
    "            name = file.readline().strip('\\n')\n",
    "            classes.append(name)\n",
    "            if not name:\n",
    "                break\n",
    "        return classes\n",
    "    \n",
    "    '''\n",
    "    target: To make inference and show image\n",
    "    name : detection\n",
    "    param[1]: image\n",
    "    param[2]: network\n",
    "    param[3]: classes\n",
    "    '''\n",
    "    def detection(self, image, net, classes):\n",
    "        # Blob to apply to input image\n",
    "        # Out[]: Return 4-dimensional matrix with NCHW dimensions order \n",
    "        blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (640, 640), swapRB=True, mean=(0, 0), crop=False)\n",
    "\n",
    "        # Set to input model\n",
    "        # Sets the new input value for the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Start time\n",
    "        t_0 = time.time()\n",
    "\n",
    "        # Unconnected layer by index\n",
    "        output_layers = net.getUnconnectedOutLayersNames()\n",
    "        # print(output_layers)\n",
    "        # ('output0',)\n",
    "\n",
    "        # Out[]: Blob for first output of specified layer\n",
    "        # Detection results\n",
    "        outputs = net.forward(output_layers)\n",
    "        # print(type(outputs))\n",
    "        # <class 'tuple'>\n",
    "        # print(outputs[0].shape)\n",
    "        # (1, 25200, 85)\n",
    "\n",
    "        # If not using `getUnconnectedOutLayersNames()`\n",
    "        # outputs = net.forward()\n",
    "        # print(type(outputs))\n",
    "        # <class 'numpy.ndarray'>\n",
    "        # print(outputs.shape)\n",
    "        # (1, 25200, 85)\n",
    "        \n",
    "        # End time\n",
    "        t_1 = time.time()\n",
    "        print('OpenCV DNN YOLOv5 Inference Time:', t_1 - t_0)\n",
    "\n",
    "        # Number of detections\n",
    "        num_detections = outputs[0].shape[1]\n",
    "        # print(num_detections)\n",
    "        # 25200\n",
    "\n",
    "        height, width = image.shape[:2]\n",
    "        # Scale\n",
    "        x_scale = width / self.image_width\n",
    "        y_scale = height / self.image_height\n",
    "\n",
    "        # Confidence\n",
    "        confidence_threshold = self.conf_threshold\n",
    "        # Score\n",
    "        score_threshold = self.score_threshold\n",
    "        # Non max suppresion \n",
    "        nms_threshold = self.nms_threshold\n",
    "\n",
    "        # List of class ids\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        bboxes = []\n",
    "\n",
    "        # Loop through detections\n",
    "        for i in range(num_detections):\n",
    "            # Detect \n",
    "            detect = outputs[0][0][i]\n",
    "            # Confidence\n",
    "            confidence = detect[4]\n",
    "\n",
    "            if confidence >= confidence_threshold:\n",
    "                # Scores of each class\n",
    "                class_score = detect[5:]\n",
    "                class_idx = np.argmax(class_score)\n",
    "\n",
    "                if (class_score[class_idx] > score_threshold):\n",
    "                    confidences.append(confidence)\n",
    "                    class_ids.append(class_idx)\n",
    "\n",
    "                    cx, cy, w, h = detect[0], detect[1], detect[2], detect[3]\n",
    "\n",
    "                    # Calculate bounding box coordinates\n",
    "                    left = int((cx - w / 2) * x_scale)\n",
    "                    top = int((cy - h / 2 ) * y_scale)\n",
    "                    width = int(w * x_scale)\n",
    "                    height = int(h * y_scale)\n",
    "\n",
    "                    box = np.array([left, top, width, height])\n",
    "                    bboxes.append(box)\n",
    "\n",
    "        # Non max suppression \n",
    "        indices = cv2.dnn.NMSBoxes(bboxes, confidences, confidence_threshold, nms_threshold)\n",
    "\n",
    "        for i in indices:\n",
    "            box = bboxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]\n",
    "            height = box[3]\n",
    "\n",
    "            # Label\n",
    "            label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
    "                            \n",
    "            # Rectangle\n",
    "            cv2.rectangle(image, (left, top), (left + width, top + height), (0, 255, 0), 3)\n",
    "            # Task for learning put text \n",
    "            cv2.putText(image, label, (left, top + 20), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.namedWindow('detection.jpg', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('detection.jpg', 900, 800)\n",
    "        cv2.imshow('detection.jpg', image)\n",
    "\n",
    "        cv2.waitKey(3000)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "def main():\n",
    "    path_to_images = 'yolov5/images'\n",
    "    path_to_model = 'yolov5/yolov5s.onnx'\n",
    "    path_to_classes = 'yolov5/coco-classes.txt'\n",
    "    \n",
    "    instance = YOLOv5(path_to_images=path_to_images, path_to_model=path_to_model, image_width=640, image_height=640, conf_threshold=0.34, score_threshold=0.38, nms_threshold=0.3, path_to_classes=path_to_classes)\n",
    "    instance()\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2ec549b-1f89-442b-9c30-a7a1273e68c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorrt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorrt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtrt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautoinit\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdriver\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcuda\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "class TRTInference:    \n",
    "    # Specify engine file path and input and output shape\n",
    "    def __init__(self, path_to_engine_file, input_shape, output_shape, path_to_class_file, conf_threshold, score_threshold, nms_threshold):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "        self.path_to_engine_file = path_to_engine_file\n",
    "        # Load engine\n",
    "        self.engine = self.load_engine(self.engine_file_path)\n",
    "        \n",
    "        # Create context\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.score_threshold = score_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "\n",
    "        # Input shape\n",
    "        self.input_shape = input_shape\n",
    "            \n",
    "        self.path_to_class_file = path_to_class_file\n",
    "        self.count = 0\n",
    "            \n",
    "        # Output shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # Read classes\n",
    "        # with open(path_to_class_file, 'r') as read_class:\n",
    "        #     self.class_labels = [line.strip() for line in read_class.readlines()]\n",
    "        with open(path_to_class_file, 'r') as read_class:\n",
    "            data = yaml.safe_load(read_class)\n",
    "            self.class_labels = [name for name in data['names'].values()]\n",
    "\n",
    "    def load_engine(self, path_to_engine_file):\n",
    "        with open(path_to_engine_file, 'rb') as f:\n",
    "            runtime = trt.Runtime(self.logger)\n",
    "            deserialized_engine = runtime.deserialize_cuda_engine(f.read())\n",
    "            return deserialized_engine\n",
    "            \n",
    "    def preprocess_image(self, image_path):\n",
    "        image_list = []\n",
    "        image_path = []\n",
    "        count = 0\n",
    "       \n",
    "        for original_image in os.listdir(image_path):\n",
    "            if original_image.endswith('.jpg') or original_image.endswith('.png') or original_image.endswith('jpeg'):\n",
    "                image_full_path = os.path.join(image_path, original_image)\n",
    "                self.image = cv2.imread(image_full_path)\n",
    "\n",
    "                self.original_h, self.original_w = self.image.shape[:2]\n",
    "           \n",
    "                # `image_size = [640, 640]`\n",
    "                self.resized_image = cv2.resize(self.image, (self.input_shape[2], self.input_shape[3]), interpolation=cv2.INTER_AREA)\n",
    "                # self.resized_image = self.resize_with_aspect_ratio(self.image, target_size=(self.input_shape[2], self.input_shape[3]))\n",
    "                image_array = np.array(self.resized_image).astype(np.float32) / 255.0\n",
    "\n",
    "                image_array = image_array.transpose((2, 0, 1))\n",
    "\n",
    "                image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "                self.resized_image_h, self.resized_image_w = self.resized_image.shape[:2]               \n",
    "\n",
    "                count += 1\n",
    "                \n",
    "                image_list.append(image_array)\n",
    "                image_path.append(image_full_path)\n",
    "\n",
    "                if count >= 12:\n",
    "                    continue\n",
    "                    \n",
    "        return  image_list, image_path\n",
    "        \n",
    "    \n",
    "    def inference_detection(self, image_path):     \n",
    "        input_list, full_img_paths = self.preprocess_image(image_path)\n",
    "\n",
    "        results = []\n",
    "        stream = cuda.Stream()\n",
    "\n",
    "        self.total_time = 0\n",
    "        self.num_frames = len(input_list)\n",
    "\n",
    "        for inputs, full_img_path in zip(input_list, full_img_paths):\n",
    "\n",
    "            # Start time\n",
    "            self.start = time.time()\n",
    "\n",
    "            inputs = np.ascontiguousarray(inputs)\n",
    "            outputs = np.empty(self.output_shape, dtype=np.float32)\n",
    "            d_inputs = cuda.mem_alloc(1 * inputs.nbytes)\n",
    "            d_outpus = cuda.mem_alloc(1 * outputs.nbytes)\n",
    "            bindings = [d_inputs, d_outpus]\n",
    "      \n",
    "            '''cuda.memcpy_htod_async(d_inputs, inputs, stream)\n",
    "\n",
    "            self.context.execute_async(bindings=bindings, stream_handle=stream.handle)\n",
    "\n",
    "            cuda.memcpy_dtoh_async(outputs, d_outpus, stream)\n",
    "\n",
    "            stream.synchronize()'''\n",
    "\n",
    "            cuda.memcpy_htod(d_inputs, inputs)\n",
    "            self.context.execute_v2(bindings)\n",
    "\n",
    "            # Copy output back to host\n",
    "            cuda.memcpy_dtoh(outputs,d_outpus)\n",
    "\n",
    "            # cuda.memcpy_htod_async(d_outpus, outputs, stream)\n",
    "            # result = self.postprocess_img(outputs)      \n",
    "         \n",
    "            d_inputs.free()\n",
    "            d_outpus.free()\n",
    "\n",
    "            # End time\n",
    "            self.end = time.time()\n",
    "            self.total_time  += (self.end - self.start)\n",
    "            self.fps = self.num_frames / self.total_time\n",
    "            self.postprocess_recognized_image(full_img_path, outputs)    \n",
    "\n",
    "        return outputs\n",
    "        \n",
    "    # Save images with detected results\n",
    "    def postprocess_recognized_image(self, image_path, outputs):\n",
    "        # image = Image.open(image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "        # image = image.cop()\n",
    "       \n",
    "        num_detections = outputs[0].shape[0]\n",
    "        # print(outputs[0][0][0])\n",
    "        \n",
    "        width, height =  image.shape[:2]\n",
    "\n",
    "        # Re-scaling\n",
    "        x_scale = width / self.resized_image_w\n",
    "        y_scale = height / self.resized_image_h\n",
    "\n",
    "        # width, height =  self.image.shape[:2]\n",
    "        conf_threshold = self.conf_threshold\n",
    "        score_threshold = self.score_threshold\n",
    "        nms_threshold = self.nms_threshold\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        bboxes = []\n",
    "        # print(outputs)\n",
    "\n",
    "        for i in range(num_detections):\n",
    "            detect = outputs[0][i]\n",
    "            # print(detect)\n",
    "            \n",
    "            conf = detect[4]\n",
    "          \n",
    "            if conf >= conf_threshold:\n",
    "                class_score = detect[5:]  \n",
    "                class_idx = np.argmax(class_score)\n",
    "\n",
    "                if (class_score[class_idx] > score_threshold):\n",
    "                    # Confidence\n",
    "                    confidences.append(getConf)\n",
    "                    class_ids.append(class_idx)\n",
    "\n",
    "                    # Get center and w, h coordinates\n",
    "                    cx, cy, w, h = detect[0], detect[1], detect[2], detect[3]\n",
    "                    # print(\"Center X\",cx, \"Center Y \", cy, \" Width\", w, \"Height: \", h)\n",
    "                    # print('*********************************************************')\n",
    "                    # print('\\n')\n",
    "                    \n",
    "                    # Left\n",
    "                    left = int((cx - w/2) * x_scale)\n",
    "\n",
    "                    # Top\n",
    "                    top = int((cy - h/2) * y_scale)\n",
    "\n",
    "                    # Width\n",
    "                    width = int(w * x_scale)\n",
    "\n",
    "                    # Height\n",
    "                    height = int(h * y_scale) \n",
    "                    \n",
    "                    # Box\n",
    "                    box = np.array([left, top, width, height])\n",
    "\n",
    "                    # Bounding boxes\n",
    "                    bboxes.append(box)\n",
    "                    \n",
    "        # print(\"output of box\")\n",
    "        # print(bboxes)\n",
    "        # get max suppresion\n",
    "        indices = cv2.dnn.NMSBoxes(bboxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "        for i in indices:\n",
    "            box = bboxes[i]\n",
    "            left = box[0] \n",
    "            top = box[1] \n",
    "            width = box[2] \n",
    "            height = box[3]\n",
    "\n",
    "            print(\"Box Left \", left , \"Box Top \", top, \"Box Width \", width, \"Box height: \", height)\n",
    "            print('*********************************************************')\n",
    "            print('\\n')\n",
    "        \n",
    "            print(self.class_labels[class_ids[i]])\n",
    "            print()\n",
    "            label = \"{}:{:.2f}\".format(self.class_labels[class_ids[i]], confidences[i])\n",
    "\n",
    "            # label2 = \"FPS: {}\".format(self.fps)\n",
    "\n",
    "            cv2.rectangle(image, (left, top),(left + width, top + height), (0, 255, 0), 3)\n",
    "            cv2.putText(image, label, (left, top - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "           \n",
    "        cv2.namedWindow('result.jpg', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('result.jpg', 900, 800)\n",
    "        cv2.imshow('result.jpg', image)\n",
    "        cv2.waitKey(2000)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    path_to_engine_file ='/home/yungshun317/workspace/py/onnx-tensorrt-inference/yolov5'\n",
    "\n",
    "    # Load the TensorRT engine\n",
    "    input_shape = (1, 3, 640, 640)\n",
    "    # output_shape = (1, 25500, 7)\n",
    "    output_shape = (1, 25200, 85)\n",
    "\n",
    "    # image_path = '/deeplearning/resnet/rose.jpeg'\n",
    "    image_path = '/home/yungshun317/workspace/py/onnx-tensorrt-inference/yolov5/images'\n",
    "    path_to_class_file = \"/home/yungshun317/workspace/py/onnx-tensorrt-inference/yolov5/yolov5/data/coco.yaml\"\n",
    "    \n",
    "    inference = TRTInference(path_to_engine_file=path_to_engine_file, input_shape=input_shape, output_shape=output_shape, path_to_class_file=path_to_class_file, conf_threshold=0.4, score_threshold=0.45, nms_threshold=0.35)\n",
    "    inference.inference_detection(image_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a7be5-64c9-4168-b4be-653a94720b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Project : yolov5 Inference on video frames\n",
    "\n",
    "'''\n",
    "# import preprocessing libraries\n",
    "\n",
    "import tensorrt as  trt\n",
    "import pycuda.autoinit\n",
    "import cv2\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "\n",
    "'''\n",
    "    Class Name: yolov5TensorRT\n",
    "    target: INIT Class params\n",
    "    param[1]: engine_file_path\n",
    "    param[2]: input_shape\n",
    "    param[3]: output_shape\n",
    "    param[4]: classes_label_file\n",
    "    param[5]: conf_threshold\n",
    "    param[6] : score_threshold\n",
    "    param[7] : nms_threshold\n",
    "\n",
    "'''\n",
    "\n",
    "class yolov5TensorRT:\n",
    "   \n",
    "    def __init__ (self, engine_file_path, input_shape, output_shape, classes_label_file, conf_threshold, score_threshold, nms_threshold):\n",
    "        \n",
    "        # Warning while engien loading\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING) # ?\n",
    "        \n",
    "        # INIt Params\n",
    "        self.engine_file_path = engine_file_path\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.classes_label_file = classes_label_file\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.score_threhold = score_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        \n",
    "        # load engine\n",
    "        self.engine = self.load_engine(self.engine_file_path)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "    \n",
    "        # read class labels of yaml\n",
    "        # return all classes \n",
    "        with open(classes_label_file, 'r') as class_read:\n",
    "            data = yaml.safe_load(class_read)\n",
    "            self.class_labels = [name for name in data['names'].values()]\n",
    "    \n",
    "    ''' \n",
    "        loading engine file and deserialize for an inference\n",
    "        param[1]: engine_file_path\n",
    "        param[out]: deserialized_engine\n",
    "    '''\n",
    "    def load_engine(self,engine_file_path):\n",
    "        with open(engine_file_path, 'rb') as f:\n",
    "            \n",
    "            runtime = trt.Runtime(self.logger)\n",
    "            engine_deserialized = runtime.deserialize_cuda_engine(f.read())\n",
    "            \n",
    "        return engine_deserialized\n",
    "    \n",
    "    '''\n",
    "        target:preprocessing video frames\n",
    "        param[1]: video_path\n",
    "        param[out]: video frames\n",
    "    '''\n",
    "    \n",
    "    def preprocess_video(self,video_path):\n",
    "        \n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # while opens, do preprocessing\n",
    "        while video.isOpened():\n",
    "            \n",
    "            ret, frame = video.read()\n",
    "            \n",
    "            frame = cv2.resize(frame, (1000, 800))\n",
    "            \n",
    "            \n",
    "            if not ret:\n",
    "                \n",
    "                break\n",
    "            \n",
    "            self.org_frame_h, self.org_frame_w = frame.shape[:2]\n",
    "            \n",
    "            \n",
    "            '''\n",
    "                NORMALIZATION FOR INFERENCE\n",
    "            '''\n",
    "            img_resized = cv2.resize(frame, (self.input_shape[2], self.input_shape[3]), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            self.resized_frame_h, self.resized_frame_w = img_resized.shape[:2]\n",
    "            \n",
    "            # convert to n umpy and divide it by float32 bit and 255.0\n",
    "            \n",
    "            img_np = np.array(img_resized).astype(np.float32) / 255.0\n",
    "            \n",
    "            img_np = np.transpose(img_np, (2, 0, 1))\n",
    "            \n",
    "            yield img_np, frame\n",
    "            \n",
    "        video.release()\n",
    "        \n",
    "    '''\n",
    "        param[1]: video path\n",
    "        param[out1]:frame\n",
    "        param[out2]:outputs \n",
    "    '''\n",
    "    def inference_detection(self,video_path):\n",
    "        \n",
    "        self.total_time = 0\n",
    "        \n",
    "        self.num_frames = 0\n",
    "        \n",
    "        \n",
    "        for inputs, frame in self.preprocess_video(video_path):\n",
    "            \n",
    "            self.num_frames += 1\n",
    "            \n",
    "            self.start = time.time()\n",
    "            \n",
    "            inputs = np.ascontiguousarray(inputs)\n",
    "            \n",
    "            outputs = np.empty(self.output_shape, dtype=np.float32)\n",
    "            \n",
    "            \n",
    "            d_inputs = cuda.mem_alloc(1 * inputs.nbytes)\n",
    "            \n",
    "            d_outputs = cuda.mem_alloc(1 * outputs.nbytes)\n",
    "\n",
    "            bindings = [d_inputs, d_outputs]\n",
    "            \n",
    "            cuda.memcpy_htod(d_inputs, inputs)\n",
    "            \n",
    "            self.context.execute_v2(bindings)\n",
    "            \n",
    "            cuda.memcpy_dtoh(outputs, d_outputs)\n",
    "            \n",
    "            d_inputs.free()\n",
    "            d_outputs.free()\n",
    "            \n",
    "            # end time \n",
    "            \n",
    "            self.end = time.time()\n",
    "            \n",
    "            self.total_time += (self.end - self.start)\n",
    "            \n",
    "            self.FPS = self.num_frames / self.total_time\n",
    "            \n",
    "            # post processing gpu results\n",
    "            \n",
    "            self.postprocessing_recognized_frames(frame, outputs)\n",
    "\n",
    "\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    target: postprocessing\n",
    "    param[1]: frame\n",
    "    param[2]: yolov5 output gpu results\n",
    "    '''    \n",
    "    def postprocessing_recognized_frames(self, frame, yolov5_output):\n",
    "        \n",
    "        \n",
    "        detections = yolov5_output[0].shape[0]\n",
    "        \n",
    "        width, height = frame.shape[:2]\n",
    "        \n",
    "        x_scale = self.org_frame_w / self.resized_frame_w\n",
    "        y_scale = self.org_frame_h / self.resized_frame_h\n",
    "        \n",
    "        conf_threshold = self.conf_threshold\n",
    "        \n",
    "        score_threshold = self.score_threhold\n",
    "        \n",
    "        nms_threshold= self.nms_threshold\n",
    "        \n",
    "        class_ids = []\n",
    "        \n",
    "        confidences = []\n",
    "        \n",
    "        bboxes = []\n",
    "        \n",
    "        \n",
    "        for i in range(detections):\n",
    "            \n",
    "            detect = yolov5_output[0][i]\n",
    "            \n",
    "            getConf = detect[4]\n",
    "            \n",
    "            if getConf >= conf_threshold:\n",
    "                \n",
    "                class_score = detect[5:]\n",
    "                \n",
    "                class_idx = np.argmax(class_score)\n",
    "                \n",
    "                if (class_score[class_idx] > score_threshold):\n",
    "                    confidences.append(getConf)\n",
    "                    \n",
    "                    class_ids.append(class_idx)\n",
    "                    \n",
    "                    # yolov5 output formats \n",
    "                    cx, cy, w, h = detect[0], detect[1], detect[2], detect[3]\n",
    "                    \n",
    "                    left = int((cx - w / 2) * x_scale)\n",
    "                    top = int((cy - h / 2) * y_scale)\n",
    "                    \n",
    "                    width =  int(w * x_scale)\n",
    "                    height = int(h * y_scale)\n",
    "                    \n",
    "                    box = np.array([left, top, width, height])\n",
    "                    \n",
    "                    bboxes.append(box)\n",
    "                    \n",
    "                    \n",
    "        \n",
    "        indices_nonmax = cv2.dnn.NMSBoxes(bboxes, confidences, conf_threshold, nms_threshold)\n",
    "        \n",
    "        for i in indices_nonmax:\n",
    "            box = bboxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]  \n",
    "            height = box[3]\n",
    "            \n",
    "            \n",
    "            label = \"{}:{:.2f}, FPS: {:.2f}\".format(self.class_labels[class_ids[i]], confidences[i], self.FPS)\n",
    "            \n",
    "            cv2.rectangle(frame, (left, top), (left + width, top+height), (0, 255, 0), 3)\n",
    "            cv2.putText(frame, label, (left, top-20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            \n",
    "            cv2.imshow('Detection.jpg', frame)\n",
    "        \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows() \n",
    "            \n",
    "            \n",
    "def main():\n",
    "    engine_file_path =  '/tensorfl_vision/TensorRT_Inference/Models/yolov5s.engine'\n",
    "    input_shape = (1, 3, 640, 640)\n",
    "    output_shape = (1, 25200, 85)\n",
    "    video_path = '/tensorfl_vision/Yolov5_Video_Inference/main_demo.mp4'\n",
    "    \n",
    "    path_to_classes = '/tensorfl_vision/TensorRT_Inference/coco.yaml'\n",
    "    \n",
    "    inference = yolov5TensorRT(engine_file_path, input_shape, output_shape,  path_to_classes, 0.4, 0.45, 0.35)\n",
    "    \n",
    "    inference.inference_detection(video_path)\n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
