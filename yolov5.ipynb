{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894a8281-5210-42ba-80d3-ef44925b33d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yungshun317/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['custom',\n",
       " 'yolov5l',\n",
       " 'yolov5l6',\n",
       " 'yolov5m',\n",
       " 'yolov5m6',\n",
       " 'yolov5n',\n",
       " 'yolov5n6',\n",
       " 'yolov5s',\n",
       " 'yolov5s6',\n",
       " 'yolov5x',\n",
       " 'yolov5x6']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.hub.list(github=\"ultralytics/yolov5\", trust_repo='check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3162e26f-152e-4e08-8c92-ff64b71a61ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/yungshun317/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'setuptools>=70.0.0'] not found, attempting AutoUpdate...\n",
      "Collecting gitpython>=3.1.30\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting setuptools>=70.0.0\n",
      "  Downloading setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setuptools, gitdb, gitpython\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.5.1\n",
      "    Uninstalling setuptools-69.5.1:\n",
      "      Successfully uninstalled setuptools-69.5.1\n",
      "Successfully installed gitdb-4.0.11 gitpython-3.1.43 setuptools-75.1.0 smmap-5.0.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 5.4s, installed 2 packages: ['gitpython>=3.1.30', 'setuptools>=70.0.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-10-8 Python-3.12.3 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4090 Laptop GPU, 16072MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1M/14.1M [00:00<00:00, 17.5MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoShape(\n",
      "  (model): DetectMultiBackend(\n",
      "    (model): DetectionModel(\n",
      "      (model): Sequential(\n",
      "        (0): Conv(\n",
      "          (conv): Conv2d(3, 32, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv(\n",
      "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (4): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): Conv(\n",
      "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (6): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): Conv(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (8): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): SPPF(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (10): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        (12): Concat()\n",
      "        (13): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (14): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (15): Upsample(scale_factor=2.0, mode='nearest')\n",
      "        (16): Concat()\n",
      "        (17): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (18): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (19): Concat()\n",
      "        (20): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (21): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (22): Concat()\n",
      "        (23): C3(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (24): Detect(\n",
      "          (m): ModuleList(\n",
      "            (0): Conv2d(128, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (2): Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Check model information from `hubconf.py` in `ultralytics/yolov5`\n",
    "# Official model\n",
    "yolo = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n",
    "print(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86dce1de-38ed-42fa-90e8-4f57788bcb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "DetectionModel                                [1, 25200, 85]            --\n",
       "‚îú‚îÄSequential: 1-1                             --                        --\n",
       "‚îÇ    ‚îî‚îÄConv: 2-1                              [1, 32, 320, 320]         --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                       [1, 32, 320, 320]         (3,488)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-2                         [1, 32, 320, 320]         --\n",
       "‚îÇ    ‚îî‚îÄConv: 2-2                              [1, 64, 160, 160]         --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-3                       [1, 64, 160, 160]         (18,496)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-4                         [1, 64, 160, 160]         --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-3                                [1, 64, 160, 160]         --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-5                         [1, 32, 160, 160]         (2,080)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-6                   [1, 32, 160, 160]         (10,304)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-7                         [1, 32, 160, 160]         (2,080)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-8                         [1, 64, 160, 160]         (4,160)\n",
       "‚îÇ    ‚îî‚îÄConv: 2-4                              [1, 128, 80, 80]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-9                       [1, 128, 80, 80]          (73,856)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-10                        [1, 128, 80, 80]          --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-5                                [1, 128, 80, 80]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-11                        [1, 64, 80, 80]           (8,256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-12                  [1, 64, 80, 80]           (82,176)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-13                        [1, 64, 80, 80]           (8,256)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-14                        [1, 128, 80, 80]          (16,512)\n",
       "‚îÇ    ‚îî‚îÄConv: 2-6                              [1, 256, 40, 40]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-15                      [1, 256, 40, 40]          (295,168)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-16                        [1, 256, 40, 40]          --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-7                                [1, 256, 40, 40]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-17                        [1, 128, 40, 40]          (32,896)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-18                  [1, 128, 40, 40]          (492,288)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-19                        [1, 128, 40, 40]          (32,896)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-20                        [1, 256, 40, 40]          (65,792)\n",
       "‚îÇ    ‚îî‚îÄConv: 2-8                              [1, 512, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-21                      [1, 512, 20, 20]          (1,180,160)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-22                        [1, 512, 20, 20]          --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-9                                [1, 512, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-23                        [1, 256, 20, 20]          (131,328)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-24                  [1, 256, 20, 20]          (655,872)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-25                        [1, 256, 20, 20]          (131,328)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-26                        [1, 512, 20, 20]          (262,656)\n",
       "‚îÇ    ‚îî‚îÄSPPF: 2-10                             [1, 512, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-27                        [1, 256, 20, 20]          (131,328)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-28                   [1, 256, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-29                   [1, 256, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-30                   [1, 256, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-31                        [1, 512, 20, 20]          (524,800)\n",
       "‚îÇ    ‚îî‚îÄConv: 2-11                             [1, 256, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-32                      [1, 256, 20, 20]          (131,328)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-33                        [1, 256, 20, 20]          --\n",
       "‚îÇ    ‚îî‚îÄUpsample: 2-12                         [1, 256, 40, 40]          --\n",
       "‚îÇ    ‚îî‚îÄConcat: 2-13                           [1, 512, 40, 40]          --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-14                               [1, 256, 40, 40]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-34                        [1, 128, 40, 40]          (65,664)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-35                  [1, 128, 40, 40]          (164,096)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-36                        [1, 128, 40, 40]          (65,664)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-37                        [1, 256, 40, 40]          (65,792)\n",
       "‚îÇ    ‚îî‚îÄConv: 2-15                             [1, 128, 40, 40]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-38                      [1, 128, 40, 40]          (32,896)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-39                        [1, 128, 40, 40]          --\n",
       "‚îÇ    ‚îî‚îÄUpsample: 2-16                         [1, 128, 80, 80]          --\n",
       "‚îÇ    ‚îî‚îÄConcat: 2-17                           [1, 256, 80, 80]          --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-18                               [1, 128, 80, 80]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-40                        [1, 64, 80, 80]           (16,448)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-41                  [1, 64, 80, 80]           (41,088)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-42                        [1, 64, 80, 80]           (16,448)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-43                        [1, 128, 80, 80]          (16,512)\n",
       "‚îÇ    ‚îî‚îÄConv: 2-19                             [1, 128, 40, 40]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-44                      [1, 128, 40, 40]          (147,584)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-45                        [1, 128, 40, 40]          --\n",
       "‚îÇ    ‚îî‚îÄConcat: 2-20                           [1, 256, 40, 40]          --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-21                               [1, 256, 40, 40]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-46                        [1, 128, 40, 40]          (32,896)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-47                  [1, 128, 40, 40]          (164,096)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-48                        [1, 128, 40, 40]          (32,896)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-49                        [1, 256, 40, 40]          (65,792)\n",
       "‚îÇ    ‚îî‚îÄConv: 2-22                             [1, 256, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-50                      [1, 256, 20, 20]          (590,080)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU: 3-51                        [1, 256, 20, 20]          --\n",
       "‚îÇ    ‚îî‚îÄConcat: 2-23                           [1, 512, 20, 20]          --\n",
       "‚îÇ    ‚îî‚îÄC3: 2-24                               [1, 512, 20, 20]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-52                        [1, 256, 20, 20]          (131,328)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-53                  [1, 256, 20, 20]          (655,872)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-54                        [1, 256, 20, 20]          (131,328)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv: 3-55                        [1, 512, 20, 20]          (262,656)\n",
       "‚îÇ    ‚îî‚îÄDetect: 2-25                           [1, 25200, 85]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-56                  --                        (229,245)\n",
       "===============================================================================================\n",
       "Total params: 7,225,885\n",
       "Trainable params: 0\n",
       "Non-trainable params: 7,225,885\n",
       "Total mult-adds (Units.GIGABYTES): 8.24\n",
       "===============================================================================================\n",
       "Input size (MB): 4.92\n",
       "Forward/backward pass size (MB): 206.37\n",
       "Params size (MB): 28.90\n",
       "Estimated Total Size (MB): 240.19\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Peek inside the nested model\n",
    "summary(yolo.model.model, input_size=(1, 3, 640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f46e6-7a8c-4238-9270-52d108994097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `export.py` for converting `yolov5s.pt` to ONNX from Ultralytics's YOLOv5 repository\n",
    "\n",
    "# !cd yolov5\n",
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "# !cd yolov5\n",
    "# !pip3 install ultralytics\n",
    "# !python export.py --weights yolov5s.pt --workspace 2048 --data data/coco.yaml --include onnx --opset 12\n",
    "\n",
    "# This will output a `yolov5s.onnx` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99e8eb-5ae3-462b-98bb-869add253cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /usr/src/tensorrt/bin/trtexec --onnx=yolov5s.onnx --saveEngine=/tensorfl_vision/Onnx-Inference-Yolov5/models_engine/yolov5s.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913a9fd-dd89-4d02-b5ca-0ce3b1f355ce",
   "metadata": {},
   "source": [
    "# YOLOv5\n",
    "\n",
    "1. `cv.dnn.readNetFromONNX()`\n",
    "2. `cv.dnn.Net.getUnconnectedOutLayersNames()`: Returns names of layers with unconnected outputs.\n",
    "3. `cv.dnn.Net.forward(outputName)`: Runs forward pass to compute output of layer with name `outputName`. By default runs forward pass for the whole network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91e821-1535-4fe6-8f9c-8a96acfa4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Packages:\n",
    "    cv2\n",
    "    numpy\n",
    "    time\n",
    "    os\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "'''\n",
    "    Target: Making Inference With Yolov5 Onnx model on Set of Images\n",
    "    Name: DetectV5Onnx\n",
    "    param[1]: imgs_path\n",
    "    param[2]: model path\n",
    "    param[3]: img_height\n",
    "    param[4]: img_width\n",
    "    param[5]: confidence threshold\n",
    "    param[6]: score threshold\n",
    "    param[7]: non max suppression threshold\n",
    "    param[8]: classes path \n",
    "\n",
    "'''\n",
    "\n",
    "class  DetectV5Onnx:\n",
    "\n",
    "    # Init Declaration\n",
    "\n",
    "    def __init__(self, imgs_path, model_path, imgs_width, imgs_height, conf_threshold, score_threshold, nms_threshold, classes_path):\n",
    "\n",
    "        # Declare params\n",
    "\n",
    "        self.imgs_path = imgs_path\n",
    "\n",
    "        #\n",
    "        self.model_path = model_path\n",
    "\n",
    "        #\n",
    "        self.imgs_width = imgs_width\n",
    "\n",
    "        #\n",
    "        self.imgs_height = imgs_height\n",
    "\n",
    "        #\n",
    "        self.conf_threshold = conf_threshold\n",
    "\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "        self.nms_threshold = nms_threshold\n",
    "\n",
    "        self.classes_path = classes_path\n",
    "\n",
    "    # python prebuilt call function\n",
    "\n",
    "    '''\n",
    "        target: Load images and Onnx Model\n",
    "        Return Type : OUT[]\n",
    "    '''\n",
    "    def __call__(self):\n",
    "\n",
    "        for img in os.listdir(self.imgs_path):\n",
    "            if img.endswith('.jpg') or img.endswith('.jpeg') or img.endswith('.png'):\n",
    "                # full absolute path to image\n",
    "                img_full_path = os.path.join(self.imgs_path, img)\n",
    "\n",
    "                # read image \n",
    "\n",
    "                image = cv2.imread(img_full_path)\n",
    "\n",
    "                # load onnx model\n",
    "\n",
    "                network = cv2.dnn.readNetFromONNX(self.model_path)\n",
    "\n",
    "                # gpu \n",
    "                network.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "\n",
    "                # config gpu    \n",
    "                network.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "                # read classess\n",
    "\n",
    "                classes = self.class_name()\n",
    "                \n",
    "                # detection function is called!\n",
    "                self.detection(image, network, classes)\n",
    "\n",
    "\n",
    "    # read labels of coco \n",
    "    '''\n",
    "        target: read class labels\n",
    "        Out: [classes list]\n",
    "    '''\n",
    "    def class_name(self):\n",
    "        \n",
    "        # list of classes\n",
    "\n",
    "        classes = []\n",
    "\n",
    "        file = open(self.classes_path, 'r')\n",
    "\n",
    "        while True:\n",
    "            name = file.readline().strip('\\n')\n",
    "            classes.append(name)\n",
    "\n",
    "            if not name:\n",
    "                break\n",
    "        return classes\n",
    "    \n",
    "\n",
    "    '''\n",
    "\n",
    "    target: To make Inference and SHow Image\n",
    "    name : detection\n",
    "    param[1]: image\n",
    "    param[2]: network\n",
    "    param[3]: classes\n",
    "\n",
    "    '''\n",
    "\n",
    "    def detection(self, img, net, classes):\n",
    "\n",
    "        # blob to apply to input image\n",
    "        #Out[] : Returnr 4-dimensional Mat with NCHW dimensions order. \n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(img, 1/255.0, (640, 640), swapRB=True, mean=(0,0), crop=False)\n",
    "\n",
    "        # set to input model\n",
    "        #Sets the new input value for the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # start time\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Unconnected Layer by Index\n",
    "        output_layers = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "        #Out[]:blob for first output of specified layer.\n",
    "        # detection results\n",
    "        outputs = net.forward(output_layers)\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        print('OpenCV DNN YOLOV5 Inference time :' , t2- t1)\n",
    "\n",
    "        # number of detections \n",
    "        # 25200\n",
    "        n_detections = outputs[0].shape[1]\n",
    "\n",
    "        height, width = img.shape[:2]\n",
    "\n",
    "        # x_scale\n",
    "\n",
    "        x_scale = width / self.imgs_width\n",
    "        y_scale = height / self.imgs_height\n",
    "\n",
    "        \n",
    "        # confidence\n",
    "        confidence_threshold = self.conf_threshold\n",
    "\n",
    "        # score\n",
    "        score_threshold = self.score_threshold\n",
    "\n",
    "        # non max suppresion \n",
    "        nms_threshold = self.nms_threshold\n",
    "\n",
    "        # lsit of class ids\n",
    "\n",
    "        class_ids = []\n",
    "\n",
    "        confidences = []\n",
    "\n",
    "        bboxes = []\n",
    "\n",
    "\n",
    "        # loop through detections\n",
    "\n",
    "        for i in range(n_detections):\n",
    "            #detect \n",
    "            detect = outputs[0][0][i]\n",
    "\n",
    "            confidence = detect[4]\n",
    "\n",
    "            if confidence >= confidence_threshold:\n",
    "                class_score = detect[5:]\n",
    "\n",
    "                class_id = np.argmax(class_score)\n",
    "\n",
    "                if (class_score[class_id] > score_threshold):\n",
    "                    \n",
    "                    confidences.append(confidence)\n",
    "\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "                    cx, cy, w, h = detect[0], detect[1], detect[2], detect[3]\n",
    "\n",
    "                    # calculate Bounding box coordinates\n",
    "\n",
    "                    left = int((cx - w / 2) *x_scale)\n",
    "\n",
    "                    top = int((cy- h / 2 ) *y_scale )\n",
    "\n",
    "                    width = int(w * x_scale)\n",
    "\n",
    "                    height = int(h * y_scale)\n",
    "\n",
    "                    box = np.array([left, top, width, height])\n",
    "\n",
    "                    bboxes.append(box)\n",
    "\n",
    "            # non max suppression \n",
    "\n",
    "        indices = cv2.dnn.NMSBoxes(bboxes, confidences, confidence_threshold, nms_threshold)\n",
    "\n",
    "        for i in indices:\n",
    "            box = bboxes[i]\n",
    "\n",
    "            left = box[0]\n",
    "\n",
    "            top = box[1]\n",
    "\n",
    "            width = box[2]\n",
    "\n",
    "            height = box[3]\n",
    "\n",
    "            # label\n",
    "\n",
    "            label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
    "            \n",
    "                # rectangle\n",
    "\n",
    "            cv2.rectangle(img, (left, top), (left + width, top + height), (0, 255, 0), 3)\n",
    "\n",
    "                    # task for learnign put text \n",
    "            cv2.putText(img, label, (left, top + 20), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.namedWindow('detection.jpg', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('detection.jpg', 900, 800)\n",
    "        cv2.imshow('detection.jpg', img)\n",
    "\n",
    "        cv2.waitKey(4000)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    image_path = '/tensorfl_vision/Onnx_YoloV5/Images'\n",
    "\n",
    "    onnx_path = '/tensorfl_vision/Onnx_YoloV5/Models/yolov5s.onnx'\n",
    "\n",
    "    classes_path = '/tensorfl_vision/Onnx_YoloV5/coco-classes.txt'\n",
    "\n",
    "    \n",
    "    instance = DetectV5Onnx(image_path, onnx_path, 640, 640, 0.34, 0.38, 0.3, classes_path)\n",
    "\n",
    "    instance()\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97893b-5b1f-41ef-9d0c-4e8fdbfd765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "class detectv5:\n",
    "    def __init__(self, img_path, model, imgs_w, imgs_h, conf_threshold, score_threshold, nms_threshold, classes_txt):\n",
    "        self.conf= conf_threshold\n",
    "        self.score=score_threshold\n",
    "        self.nms=nms_threshold\n",
    "        self.img_path= img_path\n",
    "        self.model= model\n",
    "        self.img_w= imgs_w\n",
    "        self.img_h = imgs_h\n",
    "        self.classes_file= classes_txt\n",
    "\n",
    "    # instance will be called in a way of function\n",
    "    def __call__(self):\n",
    "        \n",
    "        for img in os.listdir(self.img_path):\n",
    "            if img.endswith('.jpg') or img.endswith('.jpeg') or img.endswith('.png'):\n",
    "                img_full_path = os.path.join(self.img_path, img) \n",
    "                img = cv2.imread(img_full_path)\n",
    "                net = cv2.dnn.readNetFromONNX(self.model)\n",
    "                net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "                net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "                classes= self.class_name() # class name\n",
    "                self.detection(img, net, classes) # call detection\n",
    "        \n",
    "\n",
    "    def class_name(self):\n",
    "        classes=[]\n",
    "        file= open(self.classes_file,'r')\n",
    "        while True:\n",
    "            name=file.readline().strip('\\n')\n",
    "            classes.append(name)\n",
    "            if not name:\n",
    "                break\n",
    "        return classes\n",
    "\n",
    "    def detection(self, img, net, classes): \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Mat cv::dnn::blobFromImage \t( InputArray  \timage,\n",
    "        double  \tscalefactor = 1.0,\n",
    "        const Size &  \tsize = Size(),\n",
    "        const Scalar &  \tmean = Scalar(),\n",
    "        bool  swapRB = false,\n",
    "        bool  crop = false,\n",
    "        int    ddepth = CV_32F \n",
    "        ) \t\t\n",
    "        '''\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(img, 1/255.0, (640,640), swapRB=True, mean=(0,0,0), crop= False)\n",
    "        # set Input value for network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # start time\n",
    "        t1= time.time()\n",
    "\n",
    "        # forward :: Runs forward pass to compute output of layer with name outputName. \n",
    "\n",
    "        # Return blob for first output of specified layer.\n",
    "\n",
    "        # By default runs forward pass for the whole network.\n",
    "        #         \n",
    "        output_layers = net.getUnconnectedOutLayersNames()\n",
    "        outputs= net.forward(output_layers) #getUnconnectedOutLayersNames() : Returns names of layers with unconnected outputs. \n",
    "       # outputs = net.forward()\n",
    "        #end time\n",
    "        t2 = time.time()\n",
    "\n",
    "        \n",
    "        #(out.shape)\n",
    "        print('Opencv dnn yolov5 inference time: ', t2- t1)\n",
    "\n",
    "      \n",
    "       \n",
    "\n",
    "        n_detections= outputs[0].shape[1]  \n",
    "        \n",
    "        height, width= img.shape[:2]\n",
    "\n",
    "        # scale x and y\n",
    "        x_scale= width/self.img_w\n",
    "        y_scale= height/self.img_h\n",
    "        \n",
    "\n",
    "        conf_threshold= self.conf\n",
    "\n",
    "        score_threshold= self.score\n",
    "\n",
    "        nms_threshold=self.nms\n",
    "\n",
    "        class_ids=[]\n",
    "        confidences=[]\n",
    "        bboxes=[]\n",
    "\n",
    "        \n",
    "      \n",
    "        for i in range(n_detections):\n",
    "\n",
    "            detect=outputs[0][0][i] \n",
    "\n",
    "            confidence= detect[4]\n",
    "            #print(f\"Detection {i}: confidence {confidence}\")\n",
    "\n",
    "            if confidence >= conf_threshold:\n",
    "                class_score= detect[5:]\n",
    "              \n",
    "\n",
    "                class_id= np.argmax(class_score)\n",
    "               \n",
    "                \n",
    "                if (class_score[class_id]> score_threshold):\n",
    "\n",
    "                    confidences.append(confidence)\n",
    "\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "                    # get coordiantes of yolov5 outputs\n",
    "                    cx, cy, w, h = detect[0], detect[1], detect[2], detect[3]\n",
    "                    print(cx, cy, w, h)\n",
    "\n",
    "                    # calculate \n",
    "                    left= int((cx - w/2)* x_scale)\n",
    "\n",
    "                    top= int((cy - h/2)* y_scale)\n",
    "\n",
    "                    width = int(w * x_scale)\n",
    "\n",
    "                    height = int(h *y_scale)\n",
    "\n",
    "                    box= np.array([left, top, width, height])\n",
    "\n",
    "                    bboxes.append(box)\n",
    "\n",
    "        #np.array(score)\n",
    "        indices = cv2.dnn.NMSBoxes(bboxes, confidences, conf_threshold, nms_threshold)\n",
    "        # print(indices)\n",
    "        for i in indices:\n",
    "\n",
    "            box = bboxes[i] #box = bboxes[i[0]]\n",
    "\n",
    "            left = box[0]\n",
    "\n",
    "            top = box[1]\n",
    "\n",
    "            width = box[2]\n",
    "            \n",
    "            height = box[3] \n",
    "           \n",
    "           # cv2.rectangle(img, (left, top), (left + width, top + height), (0, 0, 255), 3)\n",
    "            \n",
    "            label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
    "\n",
    "      \n",
    "            cv2.rectangle(img, (left, top),(left + width, top + height), (0,255,0),3)\n",
    "            cv2.putText(img, label, (left, top+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2, cv2.LINE_AA)\n",
    "           \n",
    "        cv2.namedWindow('result.jpg', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('result.jpg', 900, 800)\n",
    "        cv2.imshow('result.jpg', img)\n",
    "            # cv2.imshow('output',img)    \n",
    "        cv2.waitKey(3000)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "\n",
    "'''\n",
    " if __name__ == \"__main__\":\n",
    "   parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image',help='Specify input image', default= '', type=str)\n",
    "    parser.add_argument('--weights', default='./yolov5s.onnx', type=str, help='model weights path')\n",
    "    parser.add_argument('--imgs_w', default=640,type= int, help='image size')\n",
    "    parser.add_argument('--imgs_h', default=640,type= int, help='image size')\n",
    "    parser.add_argument('--conf_thres',default= 0.7, type=float, help='confidence threshold')\n",
    "    parser.add_argument('--score_thres',type= float, default= 0.5, help='iou threshold')\n",
    "    parser.add_argument('--nms_thres',type= float, default= 0.5, help='nms threshold')\n",
    "    parser.add_argument('--classes',type=str,default='', help='class names')\n",
    "    opt= parser.parse_args()\n",
    "    instance= detectv5( opt.image, opt.weights, opt.imgs_w, opt.imgs_h, opt.conf_thres, opt.score_thres, opt.nms_thres, opt.classes)\n",
    "    instance()'''\n",
    "def main():\n",
    "    image_path = '/tensorfl_vision/Onnx-Inference-Yolov5/Images'\n",
    "    onnx_path = '/tensorfl_vision/Onnx-Inference-Yolov5/models/yolov5s.onnx'\n",
    "    classes_path = '/tensorfl_vision/Onnx-Inference-Yolov5/coco-classes.txt'\n",
    "\n",
    "    instance= detectv5(image_path, onnx_path, 640, 640, 0.34, 0.38, 0.3, classes_path)\n",
    "    instance()\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec549b-1f89-442b-9c30-a7a1273e68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import time\n",
    "class TRTInference:\n",
    "        \n",
    "    # specify engine file path and input and output shape\n",
    "    def __init__(self, engine_file_path, input_shape, output_shape, class_labels_file,conf_threshold, score_threshold, nms_threshold):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "        self.engine_file_path = engine_file_path\n",
    "            \n",
    "        ## load engine here\n",
    "        self.engine = self.load_engine(self.engine_file_path)\n",
    "\n",
    "        # craete context\n",
    "        self.context = self.engine.create_execution_context()\n",
    "\n",
    "        self.conf_threshold = conf_threshold\n",
    "\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "        self.nms_threshold = nms_threshold\n",
    "\n",
    "        # input shape\n",
    "        self.input_shape = input_shape\n",
    "            \n",
    "        self.class_labels_file =  class_labels_file\n",
    "        self.count = 0\n",
    "            \n",
    "        # output shape\n",
    "        self.output_shape = output_shape\n",
    "          \n",
    "       # with open(class_labels_file, 'r') as class_read:\n",
    "          #  self.class_labels = [line.strip() for line in class_read.readlines()]\n",
    "        with open(class_labels_file, 'r') as class_read:\n",
    "            \n",
    "            data = yaml.safe_load(class_read)\n",
    "            self.class_labels = [name for name in data['names'].values()]\n",
    "\n",
    "        \n",
    "\n",
    "    def load_engine(self, engine_file_path):\n",
    "        with open(engine_file_path, 'rb') as f:\n",
    "            runtime = trt.Runtime(self.logger)\n",
    "            engine_desentriliazed = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "            return engine_desentriliazed\n",
    "    \n",
    "        \n",
    "    def preprocess_image(self, image_path):\n",
    "        \n",
    "         #img list\n",
    "        img_list = []\n",
    "        # img path\n",
    "        img_path = []\n",
    "        count = 0\n",
    "       \n",
    "        for img_original in os.listdir(image_path):\n",
    "          \n",
    "            if img_original.endswith('.jpg') or img_original.endswith('.png') or img_original.endswith('jpeg'):\n",
    "                img_full_path = os.path.join(image_path, img_original)\n",
    "                \n",
    "                self.img = cv2.imread(img_full_path)\n",
    "\n",
    "                self.org_h, self.org_w = self.img.shape[:2]\n",
    "           \n",
    "                #img size = [640, 640]\n",
    "                \n",
    "                self.img_resized = cv2.resize(self.img, (self.input_shape[2], self.input_shape[3]), interpolation=cv2.INTER_AREA)\n",
    "                #self.img_resized = self.resize_with_aspect_ratio(self.img, target_size=(self.input_shape[2], self.input_shape[3]))\n",
    "                img_np = np.array(self.img_resized).astype(np.float32) / 255.0\n",
    "\n",
    "                img_np = img_np.transpose((2,0,1))\n",
    "\n",
    "                img_np = np.expand_dims(img_np, axis=0)\n",
    "\n",
    "                self.resized_imgH, self.resized_imgW =self.img_resized.shape[:2]               \n",
    "\n",
    "                count +=1\n",
    "                \n",
    "                img_list.append(img_np)\n",
    "                \n",
    "\n",
    "                img_path.append(img_full_path)\n",
    "\n",
    "                if count >= 12:\n",
    "                    continue\n",
    "\n",
    "                # call detection \n",
    "               \n",
    "        return  img_list, img_path\n",
    "        \n",
    "    \n",
    "    def inference_detection(self,image_path):\n",
    "        \n",
    "        input_list, full_img_paths = self.preprocess_image(image_path)\n",
    "\n",
    "        results = []\n",
    "        stream = cuda.Stream()\n",
    "\n",
    "        self.total_time = 0\n",
    "        self.num_frames = len(input_list)\n",
    "\n",
    "        for inputs ,full_img_path in zip(input_list, full_img_paths):\n",
    "\n",
    "            # start time\n",
    "            self.start = time.time()\n",
    "\n",
    "            inputs = np.ascontiguousarray(inputs)\n",
    "\n",
    "            outputs = np.empty(self.output_shape, dtype=np.float32)\n",
    "          \n",
    "\n",
    "            d_inputs = cuda.mem_alloc(1 * inputs.nbytes)\n",
    "\n",
    "            d_outpus = cuda.mem_alloc(1 * outputs.nbytes)\n",
    "\n",
    "            bindings = [d_inputs ,d_outpus]\n",
    "      \n",
    "            \n",
    "            '''cuda.memcpy_htod_async(d_inputs, inputs, stream)\n",
    "\n",
    "            self.context.execute_async(bindings=bindings, stream_handle=stream.handle)\n",
    "\n",
    "            cuda.memcpy_dtoh_async(outputs, d_outpus, stream)\n",
    "\n",
    "            stream.synchronize()'''\n",
    "\n",
    "          \n",
    "            cuda.memcpy_htod(d_inputs, inputs)\n",
    "        \n",
    "\n",
    "            self.context.execute_v2(bindings)\n",
    "\n",
    "            # copy output back to host\n",
    "            cuda.memcpy_dtoh(outputs,d_outpus)\n",
    "\n",
    "             #cuda.memcpy_htod_async(d_outpus, outputs, stream)\n",
    "           # result = self.postprocess_img(outputs)      \n",
    "         \n",
    "\n",
    "            d_inputs.free()\n",
    "\n",
    "            d_outpus.free()\n",
    "\n",
    "            # end time\n",
    "            self.end = time.time()\n",
    "            self.total_time  += (self.end- self.start)\n",
    "\n",
    "            self.fps = self.num_frames / self.total_time\n",
    "            self.postprocess_recognized_image(full_img_path, outputs)    \n",
    "\n",
    "        return outputs\n",
    "        \n",
    "    # save images with detected results\n",
    "    def postprocess_recognized_image(self, image_path, yolov5_output):\n",
    "        \n",
    "        #image = Image.open(image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "       # img = image.cop()\n",
    "       \n",
    "    \n",
    "        #for class_name in class_label:\n",
    "        detections = yolov5_output[0].shape[0]\n",
    "\n",
    "    #    print(yolov5_output[0][0][0])\n",
    "\n",
    "        width, height =  image.shape[:2]\n",
    "\n",
    "        # re-scaling\n",
    "        x_scale = width / self.resized_imgW\n",
    "        y_scale = height / self.resized_imgH\n",
    "\n",
    "        #width, height =  self.img.shape[:2]\n",
    "\n",
    "      \n",
    "\n",
    "        conf_threshold = self.conf_threshold\n",
    "        score_threshold = self.score_threshold\n",
    "        nms_threshold = self.nms_threshold\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        bboxes = []\n",
    "        #print(yolov5_output)\n",
    "    \n",
    "\n",
    "        for i in range(detections):\n",
    "\n",
    "            detect = yolov5_output[0][i]\n",
    "            #print(detect)\n",
    "            \n",
    "            getConf = detect[4]\n",
    "          \n",
    "            if getConf >= conf_threshold:\n",
    "\n",
    "            \n",
    "                class_score = detect[5:]  \n",
    "\n",
    "                class_idx = np.argmax(class_score)\n",
    "\n",
    "                if (class_score[class_idx] > score_threshold):\n",
    "\n",
    "                    # confidence\n",
    "                    confidences.append(getConf)\n",
    "                    \n",
    "                    class_ids.append(class_idx)\n",
    "\n",
    "                    #get center and w,h coordinates\n",
    "                    cx, cy, w, h = detect[0], detect[1], detect[2], detect[3]\n",
    "                   # print(\"Center X\",cx, \"Center Y \", cy, \" Width\", w, \"Height: \", h)\n",
    "                   # print('*********************************************************')\n",
    "                    #print('\\n')\n",
    "                    \n",
    "\n",
    "                    # left\n",
    "                    left = int((cx - w/2) * x_scale)\n",
    "\n",
    "                    # top\n",
    "                    top = int((cy - h/2) * y_scale)\n",
    "\n",
    "                    # width\n",
    "                    width = int(w * x_scale)\n",
    "\n",
    "                    #height\n",
    "                    height = int(h * y_scale) \n",
    "                    \n",
    "                    # box\n",
    "                    box = np.array([left, top, width, height])\n",
    "\n",
    "                    # bboxes\n",
    "                    bboxes.append(box)\n",
    "                    \n",
    "       # print(\"output of box\")\n",
    "       # print(bboxes)\n",
    "        # get max suppresion\n",
    "        indices = cv2.dnn.NMSBoxes(bboxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "        for i in indices:\n",
    "        \n",
    "            box = bboxes[i]\n",
    "            left = box[0] \n",
    "            top = box[1] \n",
    "            width = box[2] \n",
    "            height = box[3]\n",
    "\n",
    "           \n",
    "            print(\"Box Left \",left , \"Box Top \", top, \"Box Width \", width, \"Box height: \", height )\n",
    "            print('*********************************************************')\n",
    "            print('\\n')\n",
    "        \n",
    "            print(self.class_labels[class_ids[i]])\n",
    "            print()\n",
    "            label = \"{}:{:.2f}\".format(self.class_labels[class_ids[i]], confidences[i])\n",
    "\n",
    "            \n",
    "            #label2 = \"FPS:\".format(self.fps)\n",
    "\n",
    "            cv2.rectangle(image, (left, top),(left + width, top + height), (0,255,0),3)\n",
    "\n",
    "            cv2.putText(image, label, (left, top-20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2, cv2.LINE_AA)\n",
    "           \n",
    "\n",
    "        cv2.namedWindow('result.jpg', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('result.jpg', 900, 800)\n",
    "        cv2.imshow('result.jpg', image)\n",
    "        cv2.waitKey(2000)\n",
    "        cv2.destroyAllWindows()\n",
    "     \n",
    "        \n",
    "        #return image, save_img\n",
    "#engien path \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    engine_file_path ='/tensorfl_vision/Onnx-Inference-Yolov5/models_engine/yolov5s.engine'\n",
    "\n",
    "    # Load the TensorRT engine\n",
    "    input_shape = (1,3, 640, 640)\n",
    "   # output_shape = (1, 25500, 7)\n",
    "    output_shape = (1, 25200, 85)\n",
    "\n",
    "    #image_path = '/deeplearning/resnet/rose.jpeg'\n",
    "\n",
    "\n",
    "\n",
    "    image_path = '/tensorfl_vision/Onnx-Inference-Yolov5/Images'\n",
    "\n",
    "\n",
    "    path_to_class = \"/tensorfl_vision/Onnx-Inference-Yolov5/coco.yaml\"\n",
    "\n",
    "\n",
    "\n",
    "    inference = TRTInference(engine_file_path, input_shape, output_shape, path_to_class, 0.4, 0.45, 0.35)\n",
    "\n",
    "    inference.inference_detection(image_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a7be5-64c9-4168-b4be-653a94720b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Project : yolov5 Inference on video frames\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# import preprocessing libraries\n",
    "\n",
    "import tensorrt as  trt\n",
    "import pycuda.autoinit\n",
    "import cv2\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "\n",
    "'''\n",
    "    Class Name: yolov5TensorRT\n",
    "    target: INIT Class params\n",
    "    param[1]: engine_file_path\n",
    "    param[2]: input_shape\n",
    "    param[3]: output_shape\n",
    "    param[4]: classes_label_file\n",
    "    param[5]: conf_threshold\n",
    "    param[6] : score_threshold\n",
    "    param[7] : nms_threshold\n",
    "\n",
    "'''\n",
    "\n",
    "class yolov5TensorRT:\n",
    "   \n",
    "    def __init__ (self, engine_file_path, input_shape, output_shape, classes_label_file, conf_threshold, score_threshold, nms_threshold):\n",
    "        \n",
    "        # Warning while engien loading\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING) # ?\n",
    "        \n",
    "        # INIt Params\n",
    "        self.engine_file_path = engine_file_path\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.classes_label_file = classes_label_file\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.score_threhold = score_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        \n",
    "        # load engine\n",
    "        self.engine = self.load_engine(self.engine_file_path)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "    \n",
    "        # read class labels of yaml\n",
    "        # return all classes \n",
    "        with open(classes_label_file, 'r') as class_read:\n",
    "            data = yaml.safe_load(class_read)\n",
    "            self.class_labels = [name for name in data['names'].values()]\n",
    "    \n",
    "    ''' \n",
    "        loading engine file and deserialize for an inference\n",
    "        param[1]: engine_file_path\n",
    "        param[out]: deserialized_engine\n",
    "    '''\n",
    "    def load_engine(self,engine_file_path):\n",
    "        with open(engine_file_path, 'rb') as f:\n",
    "            \n",
    "            runtime = trt.Runtime(self.logger)\n",
    "            engine_deserialized = runtime.deserialize_cuda_engine(f.read())\n",
    "            \n",
    "        return engine_deserialized\n",
    "    \n",
    "    '''\n",
    "        target:preprocessing video frames\n",
    "        param[1]: video_path\n",
    "        param[out]: video frames\n",
    "    '''\n",
    "    \n",
    "    def preprocess_video(self,video_path):\n",
    "        \n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        # while opens, do preprocessing\n",
    "        while video.isOpened():\n",
    "            \n",
    "            ret, frame = video.read()\n",
    "            \n",
    "            frame = cv2.resize(frame, (1000, 800))\n",
    "            \n",
    "            \n",
    "            if not ret:\n",
    "                \n",
    "                break\n",
    "            \n",
    "            self.org_frame_h, self.org_frame_w = frame.shape[:2]\n",
    "            \n",
    "            \n",
    "            '''\n",
    "                NORMALIZATION FOR INFERENCE\n",
    "            '''\n",
    "            img_resized = cv2.resize(frame, (self.input_shape[2], self.input_shape[3]), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            self.resized_frame_h, self.resized_frame_w = img_resized.shape[:2]\n",
    "            \n",
    "            # convert to n umpy and divide it by float32 bit and 255.0\n",
    "            \n",
    "            img_np = np.array(img_resized).astype(np.float32) / 255.0\n",
    "            \n",
    "            img_np = np.transpose(img_np, (2, 0, 1))\n",
    "            \n",
    "            yield img_np, frame\n",
    "            \n",
    "        video.release()\n",
    "        \n",
    "    '''\n",
    "        param[1]: video path\n",
    "        param[out1]:frame\n",
    "        param[out2]:outputs \n",
    "    '''\n",
    "    def inference_detection(self,video_path):\n",
    "        \n",
    "        self.total_time = 0\n",
    "        \n",
    "        self.num_frames = 0\n",
    "        \n",
    "        \n",
    "        for inputs, frame in self.preprocess_video(video_path):\n",
    "            \n",
    "            self.num_frames += 1\n",
    "            \n",
    "            self.start = time.time()\n",
    "            \n",
    "            inputs = np.ascontiguousarray(inputs)\n",
    "            \n",
    "            outputs = np.empty(self.output_shape, dtype=np.float32)\n",
    "            \n",
    "            \n",
    "            d_inputs = cuda.mem_alloc(1 * inputs.nbytes)\n",
    "            \n",
    "            d_outputs = cuda.mem_alloc(1 * outputs.nbytes)\n",
    "\n",
    "            bindings = [d_inputs, d_outputs]\n",
    "            \n",
    "            cuda.memcpy_htod(d_inputs, inputs)\n",
    "            \n",
    "            self.context.execute_v2(bindings)\n",
    "            \n",
    "            cuda.memcpy_dtoh(outputs, d_outputs)\n",
    "            \n",
    "            d_inputs.free()\n",
    "            d_outputs.free()\n",
    "            \n",
    "            # end time \n",
    "            \n",
    "            self.end = time.time()\n",
    "            \n",
    "            self.total_time += (self.end - self.start)\n",
    "            \n",
    "            self.FPS = self.num_frames / self.total_time\n",
    "            \n",
    "            # post processing gpu results\n",
    "            \n",
    "            self.postprocessing_recognized_frames(frame, outputs)\n",
    "\n",
    "\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    target: postprocessing\n",
    "    param[1]: frame\n",
    "    param[2]: yolov5 output gpu results\n",
    "    '''    \n",
    "    def postprocessing_recognized_frames(self, frame, yolov5_output):\n",
    "        \n",
    "        \n",
    "        detections = yolov5_output[0].shape[0]\n",
    "        \n",
    "        width, height = frame.shape[:2]\n",
    "        \n",
    "        x_scale = self.org_frame_w / self.resized_frame_w\n",
    "        y_scale = self.org_frame_h / self.resized_frame_h\n",
    "        \n",
    "        conf_threshold = self.conf_threshold\n",
    "        \n",
    "        score_threshold = self.score_threhold\n",
    "        \n",
    "        nms_threshold= self.nms_threshold\n",
    "        \n",
    "        class_ids = []\n",
    "        \n",
    "        confidences = []\n",
    "        \n",
    "        bboxes = []\n",
    "        \n",
    "        \n",
    "        for i in range(detections):\n",
    "            \n",
    "            detect = yolov5_output[0][i]\n",
    "            \n",
    "            getConf = detect[4]\n",
    "            \n",
    "            if getConf >= conf_threshold:\n",
    "                \n",
    "                class_score = detect[5:]\n",
    "                \n",
    "                class_idx = np.argmax(class_score)\n",
    "                \n",
    "                if (class_score[class_idx] > score_threshold):\n",
    "                    confidences.append(getConf)\n",
    "                    \n",
    "                    class_ids.append(class_idx)\n",
    "                    \n",
    "                    # yolov5 output formats \n",
    "                    cx, cy, w, h = detect[0], detect[1], detect[2], detect[3]\n",
    "                    \n",
    "                    left = int((cx - w / 2) * x_scale)\n",
    "                    top = int((cy - h / 2) * y_scale)\n",
    "                    \n",
    "                    width =  int(w * x_scale)\n",
    "                    height = int(h * y_scale)\n",
    "                    \n",
    "                    box = np.array([left, top, width, height])\n",
    "                    \n",
    "                    bboxes.append(box)\n",
    "                    \n",
    "                    \n",
    "        \n",
    "        indices_nonmax = cv2.dnn.NMSBoxes(bboxes, confidences, conf_threshold, nms_threshold)\n",
    "        \n",
    "        for i in indices_nonmax:\n",
    "            box = bboxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]  \n",
    "            height = box[3]\n",
    "            \n",
    "            \n",
    "            label = \"{}:{:.2f}, FPS: {:.2f}\".format(self.class_labels[class_ids[i]], confidences[i], self.FPS)\n",
    "            \n",
    "            cv2.rectangle(frame, (left, top), (left + width, top+height), (0, 255, 0), 3)\n",
    "            cv2.putText(frame, label, (left, top-20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            \n",
    "            cv2.imshow('Detection.jpg', frame)\n",
    "        \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows() \n",
    "            \n",
    "            \n",
    "def main():\n",
    "    engine_file_path =  '/tensorfl_vision/TensorRT_Inference/Models/yolov5s.engine'\n",
    "    input_shape = (1, 3, 640, 640)\n",
    "    output_shape = (1, 25200, 85)\n",
    "    video_path = '/tensorfl_vision/Yolov5_Video_Inference/main_demo.mp4'\n",
    "    \n",
    "    path_to_classes = '/tensorfl_vision/TensorRT_Inference/coco.yaml'\n",
    "    \n",
    "    inference = yolov5TensorRT(engine_file_path, input_shape, output_shape,  path_to_classes, 0.4, 0.45, 0.35)\n",
    "    \n",
    "    inference.inference_detection(video_path)\n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
